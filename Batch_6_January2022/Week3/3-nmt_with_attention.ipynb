{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# NMT: Neural Machine Translation With Attention\n",
    "\n",
    "In this notebook we will implement a RNN based sequence-to-sequence encoder-decoder architecture to translate one language to another using PyTorch. The model is from [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473) by D Bahdanau, K Cho, Y Bengio.\n",
    ".\n",
    "\n",
    "**We will:**\n",
    "* Get the necessary data and try to represent a language\n",
    "* Preprocess the data to create pairs of utterences and create dataset splits with tokenization\n",
    "* Formulate the NMT task, and create, convert input-output pairs according to the task\n",
    "* Implement the Encoder, Attention, Decoder models in PyTorch\n",
    "* Combine these models to create Seq2Seq model its sequential forward pass.\n",
    "* Implement the training loop, train the model\n",
    "* Implement inference to translate at our will.\n",
    "* Check attention map to understand relation between input and output sentence.\n",
    "* Save and load the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Diagrams to understand NMT with Attention"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "___________\n",
    "NMT task formulation\n",
    "\n",
    "![nmt_task_formulation.drawio.png](nmt_task_formulation.drawio.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "_______\n",
    "\n",
    "General Encoder Decoder Model\n",
    "\n",
    "\n",
    "![](nmt_task_formulation-general_encoder_decoder_model.drawio.png)\n",
    "_____"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First we import all the required modules."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import math\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "from math import floor\n",
    "from typing import Tuple, List, Optional\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim, Tensor\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "from datasets import tqdm\n",
    "import requests\n",
    "import zipfile"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Setting our main device for computations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Set the random seeds for reproducability."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " # Download Data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, we download and extract the dataset we will be using."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def download_data(url, save_name=\"nmt_data.zip\"):\n",
    "    r = requests.get(url, stream=True)\n",
    "\n",
    "    bar = tqdm(\n",
    "        total=int(r.headers[\"Content-Length\"]),\n",
    "        initial=0,\n",
    "        unit='B',\n",
    "        unit_scale=True,\n",
    "    )\n",
    "\n",
    "    with open(save_name, \"wb\") as f:\n",
    "        for chunk in r.iter_content(chunk_size=1024):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "                bar.update(1024)\n",
    "    bar.close()\n",
    "\n",
    "def extract_data(saved_name=\"nmt_data.zip\"):\n",
    "    zipfile.ZipFile(saved_name).extractall()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0.00/2.88M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0d08acd0504a4020b9d83d784152d554"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_url = \"https://download.pytorch.org/tutorial/data.zip\"\n",
    "download_data(data_url)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "extract_data()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " # Representing a language\n",
    "When we check our data, we would see pairs of sentences in two languages. However, our models are not capable of directly interpreting words, otherwise we would have no problem at all :)\n",
    "\n",
    "Instead we will first represent a word in a language with a specific index. There other ways to represent a language such as character level encoding but in this notebook, we are going to use word level encoding.\n",
    "\n",
    "To do that we need to have 2 dictionaries word2index, given a word returns its index, and index2word, given an index returns its word.\n",
    "\n",
    "Additionally, we are keeping track of word counts because we might also want replace less frequent with <unknown> token."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "___________\n",
    "![lang_representation.drawio.png](lang_representation.drawio.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "sos_token_index = 1\n",
    "eos_token_index = 2\n",
    "pad_token_index = 0\n",
    "sos_token = '<start>'\n",
    "eos_token = '<end>'\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {sos_token:sos_token_index, eos_token: eos_token_index}\n",
    "        self.word2count = {sos_token:1, eos_token: 1}\n",
    "        self.index2word = {sos_token_index: sos_token, eos_token_index: eos_token}\n",
    "        self.n_words = 3 # Count SOS and EOS\n",
    "\n",
    "    def add_sentence(self, sentence: str):\n",
    "        for word in sentence.split(' '):\n",
    "            self.add_word(word)\n",
    "\n",
    "    def add_word(self, word: str):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Before creating a language we need to preprocess the data!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def unicode_to_ascii(sentence: str) -> str:\n",
    "    \"\"\"\n",
    "     NFD, 'Normal Form Decomposed' gives you decomposed, combined characters.\n",
    "    Mn, Nonspacing mark\n",
    "    :param sentence:\n",
    "    :return: ascii sentence\n",
    "    \"\"\"\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', sentence)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Because the files are all in Unicode, we will convert Unicode characters to ASCII, lowercase everything, and remove most punctuation."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def normalize_string(s: str) -> str:\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    # creating a space between a word and the punctuyouation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "    s = re.sub(r\"([?.!,多])\", r\" \\1 \", s)\n",
    "    s = re.sub(r'[\" \"]+', \" \", s)\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\", \"多\")\n",
    "    s = re.sub(r\"[^a-zA-Z?.!,多]+\", \" \", s)\n",
    "    return s.strip()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'may i borrow this book ?'\n",
      "b'\\xc2\\xbf puedo tomar prestado este libro ?'\n",
      "b'puis je emprunter ce livre ?'\n"
     ]
    }
   ],
   "source": [
    "sentences = [u\"May I borrow this book?\",\n",
    "             u\"多Puedo tomar prestado este libro?\",\n",
    "             u\"Puis-je emprunter ce livre?\"]\n",
    "for sentence in sentences:\n",
    "    print(normalize_string(sentence).encode('utf-8'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset creation and tokenization"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To read the data file, we will divide it into lines and then into pairs. Also, we need to be sure that we are normalizing every expression in each language."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def read_langs(lang1: str, lang2: str, num_examples: Optional[int] = None) -> (Lang, Lang, List[Tuple[str]]):\n",
    "    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8')\\\n",
    "        .read()\\\n",
    "        .strip()\\\n",
    "        .split('\\n')\n",
    "    if num_examples:\n",
    "        lines=lines[:num_examples]\n",
    "    print('Read %s lines' % len(lines))\n",
    "    pairs = [[normalize_string(s) for s in l.split('\\t')[:2]] for l in lines]\n",
    "    input_lang = Lang(lang1)\n",
    "    output_lang = Lang(lang2)\n",
    "    return input_lang, output_lang, pairs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We'll limit the dataset to only very short and easy sentences because there are a lot of sample sentences and we want to train something very quick.\n",
    "The maximum length here is 8 words (including ending punctuation), and additionally we'are adding phrases that begin with \"I am\" or \"He is,\" as a further ease. However, that is optional and we are leaving for you to play with them."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "outputs": [],
   "source": [
    "MAX_LENGTH = 11\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "def filter_pair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH\n",
    "    # and p[0].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "def filter_pairs(pairs):\n",
    "    return [pair for pair in pairs if filter_pair(pair)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The entire data preparation procedure is as follows:\n",
    "- Read a text file and divide it into lines, then divide the lines into pairs.\n",
    "- Text should be normalized and filtered based on length (and substance).\n",
    "- Create Language (word dictionaries) from pairs of sentences."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "outputs": [],
   "source": [
    "def prepare_data(lang1: str, lang2: str, num_examples: Optional[int] = None):\n",
    "    input_lang, output_lang, pairs = read_langs(lang1, lang2, num_examples)\n",
    "    print('Read %s sentence pairs' % len(pairs))\n",
    "    pairs = filter_pairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    for pair in pairs:\n",
    "        input_lang.add_sentence(pair[0])\n",
    "        output_lang.add_sentence(pair[1])\n",
    "    print('Counted words:')\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In out example we are translating English to French"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 135842 lines\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 106364 sentence pairs\n",
      "Counted words:\n",
      "eng 10673\n",
      "fra 17859\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, pairs = prepare_data('eng', 'fra', None)\n",
    "num_examples=len(pairs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Checking the longest sentences for each language, although we put limitation we need to be sure of our max lengths for later stages, just for the sake of efficiency :) And it is a good way to familiarize ourselves with the data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i ll be there at five p . m .\n",
      "qui que ce soit est il a la maison ?\n",
      "12 11\n"
     ]
    }
   ],
   "source": [
    "longest_input_sentence = sorted(map(lambda x: x[0], pairs), key=lambda x: len(x.split(' ')), reverse=True)[0]\n",
    "print(longest_input_sentence)\n",
    "# TODO: explain why we need those +2 and +1's\n",
    "longest_input_length = len(longest_input_sentence.split(' ')) + 2\n",
    "longest_output = sorted(map(lambda x: x[1], pairs), key=lambda x: len(x.split(' ')), reverse=True)[0]\n",
    "print(longest_output)\n",
    "longest_output_length = len(longest_output.split(' ')) + 1\n",
    "print(longest_input_length, longest_output_length)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i really enjoyed myself .', 'je me suis vraiment bien amusee .']\n"
     ]
    }
   ],
   "source": [
    "print(random.choice(pairs))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### sentence - index - tensor conversions\n",
    "To train, we will require an input tensor (indexes of the words in the input phrase) and a target tensor for each pair (indexes of the words in the target sentence). Then we need to be able to go back to phrase from indexes alone to interpret our models outputs."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "outputs": [],
   "source": [
    "def indexes_from_sentence(lang: Lang, sentence: str) -> List[int]:\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "outputs": [],
   "source": [
    "def sentence_from_indexes(lang: Lang, indexes):\n",
    "    return ' '.join([lang.index2word[index] for index in indexes])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "outputs": [],
   "source": [
    "def tensor_from_sentence(lang, sentence):\n",
    "    indexes=indexes_from_sentence(lang, sentence)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "outputs": [],
   "source": [
    "def sentence_from_tensor(lang, tensor: torch.Tensor):\n",
    "    return sentence_from_indexes(lang, tensor.tolist())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "outputs": [],
   "source": [
    "def tensors_from_pair(input_lang, output_lang, pair):\n",
    "    input_tensor=tensor_from_sentence(input_lang,pair[0])\n",
    "    output_tensor=tensor_from_sentence(output_lang,pair[1])\n",
    "    return input_tensor, output_tensor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "outputs": [],
   "source": [
    "def add_sos_eos_tokens_to_pair(pair):\n",
    "    source = ' '.join([sos_token, pair[0], eos_token])\n",
    "    target = ' '.join([pair[1], eos_token])\n",
    "    return source, target"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We are checking whether our methods are able to convert and convert back between indexes and tensors and words truthfully."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the driver was charged with speeding .\n",
      "tensor([ 525, 3228,  232, 7298,  773, 5689,    4], device='cuda:0')\n",
      "the driver was charged with speeding .\n"
     ]
    }
   ],
   "source": [
    "sample_input = random.choice(pairs)[0]\n",
    "print(sample_input)\n",
    "sample_input_tensor = tensor_from_sentence(input_lang, sample_input)\n",
    "print(sample_input_tensor)\n",
    "print(sentence_from_tensor(input_lang, sample_input_tensor))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PyTorch Dataset and Dataloader creation\n",
    "- Code for processing data samples may become cluttered and difficult to maintain; ideally, we want our dataset code to be separated from our model training code for greater readability and modularity.\n",
    "- PyTorch includes two data primitives: torch.utils.data.DataLoader and torch.utils.data.\n",
    "- The samples and their labels are stored in Dataset, and DataLoader wraps an iterable around the Dataset to provide simple access to the samples."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**NMT Dataset:**\n",
    "- Accesses pairs by their indexes and adds SOS & EOS tokens to phrases(sentences).\n",
    "- Then tensors from word indexes are created. Since we want to process our data in batches, they need to be in exact dimensions (elements in the same batch).\n",
    "- We can further improve our approach to that. Currently, every batch is capped to max sentence length regarding its language. In the improved case, one can compute the max length for each batch and then pad accordingly.\n",
    "- We can check whether our data is corrupted or mistakes we make with exception handling."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "outputs": [],
   "source": [
    "class NMTDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 input_lang,\n",
    "                 output_lang,\n",
    "                 pairs,\n",
    "                 max_input_length: int,\n",
    "                 max_output_length: int):\n",
    "        self.input_lang = input_lang\n",
    "        self.output_lang = output_lang\n",
    "        self.pairs = pairs\n",
    "        self.max_input_length = max_input_length\n",
    "        self.max_output_length = max_output_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(pairs)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        try:\n",
    "            pair = pairs[item]\n",
    "            source, target = add_sos_eos_tokens_to_pair(pair)\n",
    "            input_tensor, output_tensor = tensors_from_pair(input_lang, output_lang, (source, target))\n",
    "            padded_input_tensor = torch.full([self.max_input_length], pad_token_index, dtype=torch.long, device=device)\n",
    "            padded_input_tensor[:len(input_tensor)] = input_tensor\n",
    "            padded_output_tensor = torch.full([self.max_output_length], pad_token_index, dtype=torch.long, device=device)\n",
    "            padded_output_tensor[:len(output_tensor)] = output_tensor\n",
    "        except Exception as e:\n",
    "            print('index', item)\n",
    "            print(pairs[item])\n",
    "            raise e\n",
    "        return {'source': padded_input_tensor, 'target': padded_output_tensor}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It is vital to have a train, validation, and test datasets. Here we are creating them with random splits."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "outputs": [],
   "source": [
    "dataset=NMTDataset(input_lang, output_lang, pairs, longest_input_length, longest_output_length)\n",
    "train_size=floor(num_examples*0.8)\n",
    "val_size= floor(num_examples*0.1)\n",
    "test_size=num_examples - train_size - val_size\n",
    "train_val_test_split = (train_size, val_size, test_size)\n",
    "\n",
    "data_train, data_val, data_test = random_split(\n",
    "                dataset, train_val_test_split, generator=torch.Generator().manual_seed(42)\n",
    "            )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "outputs": [],
   "source": [
    "batch_size=128"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " PyTorch Dataloader represents a Python iterable over a dataset, with support for\n",
    " - map-style and iterable-style datasets,\n",
    " - customizing data loading order,\n",
    " - automatic batching,\n",
    " - single- and multi-process data loading,\n",
    " - automatic memory pinning.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "outputs": [],
   "source": [
    "train_dataloader=DataLoader(\n",
    "            dataset=data_train,\n",
    "            batch_size=batch_size,\n",
    "            num_workers=0,\n",
    "            pin_memory=False,\n",
    "            shuffle=True,\n",
    "        )\n",
    "val_dataloader=DataLoader(\n",
    "            dataset=data_val,\n",
    "            batch_size=batch_size,\n",
    "            num_workers=0,\n",
    "            pin_memory=False,\n",
    "            shuffle=False,\n",
    "        )\n",
    "test_dataloader=DataLoader(\n",
    "            dataset=data_test,\n",
    "            batch_size=batch_size,\n",
    "            num_workers=0,\n",
    "            pin_memory=False,\n",
    "            shuffle=False,\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': tensor([   1,  525, 2749, 8734,  202,   96, 8735,    4,    2,    0,    0,    0],\n",
      "       device='cuda:0'), 'target': tensor([  115,   499,   353,  3347,   358,   438, 15027,    15,     2,     0,\n",
      "            0], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "print(random.choice(dataset))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We are controlling whether batches are in an expected shape. In this case, the max length cap is 11, so the longest sentence has 10 \"words.\" For input, we add SOS and EOS. For output, we are only counting the EOS token."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 12])\n",
      "torch.Size([128, 11])\n"
     ]
    }
   ],
   "source": [
    "sample_batch = next(iter(train_dataloader))\n",
    "print(sample_batch['source'].shape)\n",
    "print(sample_batch['target'].shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# The Model and Attention"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Encoder\n",
    "\n",
    "First, we'll build the encoder. Similar to the previous model, we only use a single layer GRU, however we now use a *bidirectional RNN*. With a bidirectional RNN, we have two RNNs in each layer. A *forward RNN* going over the sentence from left to right (shown below in green), and a *backward RNN* going over the sentence from right to left (yellow). All we need to do in code is set `bidirectional = True` and then pass the embedded sentence to the RNN as before.\n",
    "\n",
    "![](encoder.drawio.png)\n",
    "\n",
    "We now have:\n",
    "\n",
    "$$\\begin{align*}\n",
    "h_t^\\rightarrow &= \\text{EncoderGRU}^\\rightarrow(x_t^\\rightarrow,h_t^\\rightarrow)\\\\\n",
    "h_t^\\leftarrow &= \\text{EncoderGRU}^\\leftarrow(x_t^\\leftarrow,h_t^\\leftarrow)\n",
    "\\end{align*}$$\n",
    "\n",
    "Where $x_0^\\rightarrow = \\text{<sos>}, x_1^\\rightarrow = \\text{guten}$ and $x_0^\\leftarrow = \\text{<eos>}, x_1^\\leftarrow = \\text{morgen}$.\n",
    "\n",
    "we only pass an input (`embedded`) to the RNN, which tells PyTorch to initialize both the forward and backward initial hidden states ($h_0^\\rightarrow$ and $h_0^\\leftarrow$, respectively) to a tensor of all zeros. We'll also get two context vectors, one from the forward RNN after it has seen the final word in the sentence, $z^\\rightarrow=h_T^\\rightarrow$, and one from the backward RNN after it has seen the first word in the sentence, $z^\\leftarrow=h_T^\\leftarrow$.\n",
    "\n",
    "The RNN returns `outputs` and `hidden`.\n",
    "\n",
    "`outputs` is of size **[src sent len, batch size, hid dim * num directions]** where the first `hid_dim` elements in the third axis are the hidden states from the top layer forward RNN, and the last `hid_dim` elements are hidden states from the top layer backward RNN. You can think of the third axis as being the forward and backward hidden states stacked on top of each other, i.e. $h_1 = [h_1^\\rightarrow; h_{T}^\\leftarrow]$, $h_2 = [h_2^\\rightarrow; h_{T-1}^\\leftarrow]$ and we can denote all stacked encoder hidden states as $H=\\{ h_1, h_2, ..., h_T\\}$.\n",
    "\n",
    "`hidden` is of size **[n layers * num directions, batch size, hid dim]**, where **[-2, :, :]** gives the top layer forward RNN hidden state after the final time-step (i.e. after it has seen the last word in the sentence) and **[-1, :, :]** gives the top layer backward RNN hidden state after the final time-step (i.e. after it has seen the first word in the sentence).\n",
    "\n",
    "As the decoder is not bidirectional, it only needs a single context vector, $z$, to use as its initial hidden state, $s_0$, and we currently have two, a forward and a backward one ($z^\\rightarrow=h_T^\\rightarrow$ and $z^\\leftarrow=h_T^\\leftarrow$, respectively). We solve this by concatenating the two context vectors together, passing them through a linear layer, $g$, and applying the $\\tanh$ activation function.\n",
    "\n",
    "$$z=\\tanh(g(h_T^\\rightarrow, h_T^\\leftarrow)) = \\tanh(g(z^\\rightarrow, z^\\leftarrow)) = s_0$$\n",
    "\n",
    "As we want our model to look back over the whole of the source sentence we return `outputs`, the stacked forward and backward hidden states for every token in the source sentence. We also return `hidden`, which acts as our initial hidden state in the decoder. [^fn1]\n",
    "\n",
    "[^fn1]: https://github.com/SethHWeidman/pytorch-seq2seq/"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim: int,\n",
    "                 emb_dim: int,\n",
    "                 enc_hid_dim: int,\n",
    "                 dec_hid_dim: int,\n",
    "                 dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        self.dropout = dropout\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
    "        self.d = (1 if not self.rnn.bidirectional else 2) * self.rnn.num_layers\n",
    "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor # B, L\n",
    "                ) -> Tuple[Tensor]:\n",
    "        embedded = self.dropout(self.embedding(src)) # B, L, E\n",
    "        outputs, hidden = self.rnn(embedded.permute(1, 0, 2)) # L, B, d*e_H - d, B, e_H\n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))) # B, d_H\n",
    "        return outputs, hidden"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units)\n",
      " torch.Size([16, 32, 128])\n",
      "Encoder Hidden state shape: (batch size, decoder hidden size)\n",
      " torch.Size([32, 64])\n"
     ]
    }
   ],
   "source": [
    "sample_input = torch.randint(1, 100, (32, 16),  device=device)\n",
    "encoder = Encoder(input_lang.n_words, emb_dim=64, enc_hid_dim=64, dec_hid_dim=64).to(device=device)\n",
    "sample_encoder_output, sample_encoder_hidden = encoder(sample_input)\n",
    "print (f'Encoder output shape: (batch size, sequence length, units)\\n {sample_encoder_output.shape}')\n",
    "print (f'Encoder Hidden state shape: (batch size, decoder hidden size)\\n {sample_encoder_hidden.shape}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Attention\n",
    "\n",
    "Next up is the attention layer. This will take in the previous hidden state of the decoder, $s_{t-1}$, and all of the stacked forward and backward hidden states from the encoder, $H$. The layer will output an attention vector, $a_t$, that is the length of the source sentence, each element is between 0 and 1 and the entire vector sums to 1.\n",
    "\n",
    "Intuitively, this layer takes what we have decoded so far, $s_{t-1}$, and all of what we have encoded, $H$, to produce a vector, $a_t$, that represents which words in the source sentence we should pay the most attention to in order to correctly predict the next word to decode, $\\hat{y}_{t+1}$.\n",
    "\n",
    "First, we calculate the *energy* between the previous decoder hidden state and the encoder hidden states. As our encoder hidden states are a sequence of $T$ tensors, and our previous decoder hidden state is a single tensor, the first thing we do is `repeat` the previous decoder hidden state $T$ times. We then calculate the energy, $E_t$, between them by concatenating them together and passing them through a linear layer (`attn`) and a $\\tanh$ activation function.\n",
    "\n",
    "$$E_t = \\tanh(\\text{attn}(s_{t-1}, H))$$\n",
    "\n",
    "This can be thought of as calculating how well each encoder hidden state \"matches\" the previous decoder hidden state.\n",
    "\n",
    "We currently have a **[dec hid dim, src sent len]** tensor for each example in the batch. We want this to be **[src sent len]** for each example in the batch as the attention should be over the length of the source sentence. This is achieved by multiplying the `energy` by a **[1, attn dim]** tensor, $v$.\n",
    "\n",
    "$$\\hat{a}_t = v E_t$$\n",
    "\n",
    "We can think of this as calculating a weighted sum of the \"match\" over all `dec_hid_dem` elements for each encoder hidden state, where the weights are learned (as we learn the parameters of $v$).\n",
    "\n",
    "Finally, we ensure the attention vector fits the constraints of having all elements between 0 and 1 and the vector summing to 1 by passing it through a $\\text{softmax}$ layer.\n",
    "\n",
    "$$a_t = \\text{softmax}(\\hat{a_t})$$\n",
    "\n",
    "This gives us the attention over the source sentence!\n",
    "\n",
    "Graphically, this looks something like below. This is for calculating the very first attention vector, where $s_{t-1} = s_0 = z$. The green/yellow blocks represent the hidden states from both the forward and backward RNNs, and the attention computation is all done within the pink block. [^fn2]\n",
    "\n",
    "[^fn2]: https://github.com/SethHWeidman/pytorch-seq2seq/\n",
    "\n",
    "![](attention.drawio.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 enc_hid_dim: int,\n",
    "                 dec_hid_dim: int,\n",
    "                 attn_dim: int):\n",
    "        super().__init__()\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        self.attn_in = (enc_hid_dim * 2) + dec_hid_dim\n",
    "        self.attn = nn.Linear(self.attn_in, attn_dim)\n",
    "        self.v = nn.Parameter(torch.rand(attn_dim))\n",
    "\n",
    "    def forward(self,\n",
    "                decoder_hidden: torch.Tensor,  # [batch size, dec hid dim]\n",
    "                encoder_outputs: torch.Tensor  #  [src len, batch size, enc hid dim * 2]\n",
    "                ) -> torch.Tensor:\n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #encoder_outputs = [src sent len, batch size, enc hid dim * 2]\n",
    "\n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "\n",
    "        #repeat decoder hidden state src_len times\n",
    "        repeated_decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "\n",
    "        #decoder_hidden = [batch size, src sent len, dec hid dim]\n",
    "        #encoder_outputs = [batch size, src sent len, enc hid dim * 2]\n",
    "\n",
    "        # Step 1: to enable feeding through \"self.attn\" pink box above, concatenate\n",
    "        # `repeated_decoder_hidden` and `encoder_outputs`:\n",
    "        # torch.cat((hidden, encoder_outputs), dim = 2) has shape\n",
    "        # [batch_size, seq_len, enc_hid_dim * 2 + dec_hid_dim]\n",
    "\n",
    "        # Step 2: feed through self.attn to end up with:\n",
    "        # [batch_size, seq_len, attn_dim]\n",
    "\n",
    "        # Step 3: feed through tanh\n",
    "\n",
    "        energy = torch.tanh(self.attn(torch.cat((\n",
    "            repeated_decoder_hidden,\n",
    "            encoder_outputs),\n",
    "            dim = 2)))\n",
    "\n",
    "        #energy = [batch size, src sent len, attn_dim]\n",
    "\n",
    "        energy = energy.permute(0, 2, 1)\n",
    "\n",
    "        #energy = [batch size, attn_dim, src sent len]\n",
    "\n",
    "        #v = [attn_dim]\n",
    "\n",
    "        v = self.v.repeat(batch_size, 1).unsqueeze(1)\n",
    "\n",
    "        #v = [batch size, 1, attn_dim]\n",
    "\n",
    "        # High level: energy a function of both encoder element outputs and most recent decoder hidden state,\n",
    "        # of shape attn_dim x enc_seq_len for each observation\n",
    "        # v, being 1 x attn_dim, transforms this into a vector of shape 1 x enc_seq_len for each observation\n",
    "        # Then, we take the softmax over these to get the output of the attention function\n",
    "\n",
    "        attention = torch.bmm(v, energy).squeeze(1)\n",
    "\n",
    "        #attention= [batch size, src len]\n",
    "\n",
    "        return F.softmax(attention, dim=1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Decoder\n",
    "\n",
    "Next up is the decoder.\n",
    "\n",
    "The decoder contains the attention layer, `attention`, which takes the previous hidden state, $s_{t-1}$, all of the encoder hidden states, $H$, and returns the attention vector, $a_t$.\n",
    "\n",
    "We then use this attention vector to create a weighted source vector, $w_t$, denoted by `weighted`, which is a weighted sum of the encoder hidden states, $H$, using $a_t$ as the weights.\n",
    "\n",
    "$$w_t = a_t H$$\n",
    "\n",
    "The input word (that has been embedded), $y_t$, the weighted source vector, $w_t$, and the previous decoder hidden state, $s_{t-1}$, are then all passed into the decoder RNN, with $y_t$ and $w_t$ being concatenated together.\n",
    "\n",
    "$$s_t = \\text{DecoderGRU}(y_t, w_t, s_{t-1})$$\n",
    "\n",
    "We then pass $y_t$, $w_t$ and $s_t$ through the linear layer, $f$, to make a prediction of the next word in the target sentence, $\\hat{y}_{t+1}$. This is done by concatenating them all together.\n",
    "\n",
    "$$\\hat{y}_{t+1} = f(y_t, w_t, s_t)$$\n",
    "\n",
    "The image below shows decoding the first word in an example translation.\n",
    "\n",
    "![](decoder.drawio.png)\n",
    "\n",
    "The green/yellow blocks show the forward/backward encoder RNNs which output $H$, the red block shows the context vector, $z = h_T = \\tanh(g(h^\\rightarrow_T,h^\\leftarrow_T)) = \\tanh(g(z^\\rightarrow, z^\\leftarrow)) = s_0$, the blue block shows the decoder RNN which outputs $s_t$, the purple block shows the linear layer, $f$, which outputs $\\hat{y}_{t+1}$ and the orange block shows the calculation of the weighted sum over $H$ by $a_t$ and outputs $w_t$. Not shown is the calculation of $a_t$.[^fn3]\n",
    "\n",
    "[^fn3]: https://github.com/SethHWeidman/pytorch-seq2seq/"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 output_dim: int,\n",
    "                 emb_dim: int,\n",
    "                 enc_hid_dim: int,\n",
    "                 dec_hid_dim: int,\n",
    "                 attention: nn.Module,\n",
    "                 dropout: float = 0.1, ):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.dropout = dropout\n",
    "        self.attention = attention\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
    "        self.out = nn.Linear(self.attention.attn_in + emb_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def _weighted_encoder_rep(self,\n",
    "                              decoder_hidden: Tensor,\n",
    "                              encoder_outputs: Tensor) -> Tensor:\n",
    "        a = self.attention(decoder_hidden, encoder_outputs) # B, L\n",
    "        a = a.unsqueeze(1) # B, 1, L\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2) # B, L, d*e_H\n",
    "        weighted_encoder_rep = torch.bmm(a, encoder_outputs) # B, 1, d*e_H\n",
    "        weighted_encoder_rep = weighted_encoder_rep.permute(1, 0, 2) # 1, B, d*e_H\n",
    "        return weighted_encoder_rep, a.squeeze(1)\n",
    "\n",
    "    def forward(self,\n",
    "                input: Tensor,\n",
    "                decoder_hidden: Tensor,\n",
    "                encoder_outputs: Tensor) -> Tuple[Tensor]:\n",
    "        input = input.unsqueeze(0) # 1, B\n",
    "        embedded = self.dropout(self.embedding(input)) # 1, B, E\n",
    "        weighted_encoder_rep, attn_weights = self._weighted_encoder_rep(decoder_hidden,\n",
    "                                                          encoder_outputs)\n",
    "        rnn_input = torch.cat((embedded, weighted_encoder_rep), dim=2)\n",
    "        output, decoder_hidden = self.rnn(rnn_input, decoder_hidden.unsqueeze(0))\n",
    "        embedded = embedded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weighted_encoder_rep = weighted_encoder_rep.squeeze(0)\n",
    "        output = self.out(torch.cat((output,\n",
    "                                     weighted_encoder_rep,\n",
    "                                     embedded), dim=1))\n",
    "\n",
    "        return output, decoder_hidden.squeeze(0), attn_weights"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch size, output_vocab_size)\n",
      " torch.Size([32, 17859])\n",
      "Decoder Hidden state shape: (batch size, decoder_hidden)\n",
      " torch.Size([32, 64])\n",
      "Decoder Attention Weights shape: (batch size, max_input_length)\n",
      " torch.Size([32, 16])\n"
     ]
    }
   ],
   "source": [
    "# Decoder with Attention Results\n",
    "sample_input = torch.randint(0, 1, (32, 1),  device=device)[:, 0]\n",
    "decoder = Decoder(output_dim= output_lang.n_words,\n",
    "                 emb_dim=64,\n",
    "                 enc_hid_dim=64,\n",
    "                 dec_hid_dim=64,\n",
    "                 attention=Attention(64, 64, 8)).to(device=device)\n",
    "sample_output, sample_hidden, attn_weights = decoder.forward(sample_input, sample_encoder_hidden, sample_encoder_output)\n",
    "print (f'Decoder output shape: (batch size, output_vocab_size)\\n {sample_output.shape}')\n",
    "print (f'Decoder Hidden state shape: (batch size, decoder_hidden)\\n {sample_hidden.shape}')\n",
    "print (f'Decoder Attention Weights shape: (batch size, max_input_length)\\n {attn_weights.shape}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Combining Encoder + Decoder to create Seq2Seq model\n",
    "Now we are going to combine every model component created into one model that gets the source and outputs the target. Although encoder is used as is decoding process must be realized carefully.\n",
    "\n",
    "Decoding steps:\n",
    "- the `outputs` tensor is created to hold all predictions, $\\hat{Y}$\n",
    "- the source sequence, $X$, is fed into the encoder to receive $z$ and $H$\n",
    "- the initial decoder hidden state is set to be the `context` vector, $s_0 = z = h_T$\n",
    "- we use a batch of `<sos>` tokens as the first `input`, $y_1$\n",
    "- we then decode within a loop:\n",
    "  - inserting the input token $y_t$, previous hidden state, $s_{t-1}$, and all encoder outputs, $H$, into the decoder\n",
    "  - receiving a prediction, $\\hat{y}_{t+1}$, and a new hidden state, $s_t$\n",
    "  - we then decide if we are going to teacher force or not, setting the next input as appropriate\n",
    "\n",
    "During the decoding process method called **Teacher Forcing** is being used. This method is the technique where the target word is passed as the following input to the decoder so that our model makes use of ground truth even in the later stages of decoding. Otherwise, it is only fed with its predictions during the decoding steps, which are initially totally off."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self,\n",
    "                source: torch.Tensor,\n",
    "                target: torch.Tensor,\n",
    "                teacher_forcing_ratio: float = 0.5) -> torch.Tensor:\n",
    "        batch_size = source.shape[0]\n",
    "        max_len = target.shape[1]\n",
    "        outputs = torch.zeros(batch_size, max_len, decoder.output_dim).to(device)\n",
    "        encoder_output, hidden = self.encoder(source)\n",
    "        # first input to the decoder is the <sos> token\n",
    "        output = Tensor([sos_token_index]).long().to(device=device).repeat(batch_size)\n",
    "        for t in range(max_len):\n",
    "            output, hidden, _ = self.decoder.forward(output, hidden, encoder_output)\n",
    "            outputs[:, t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top_v, top_i= output.topk(1)\n",
    "            if t + 1 is not max_len:\n",
    "                output = (target[:, t] if teacher_force else top_i.squeeze(1))\n",
    "\n",
    "        return outputs # B, L, V_out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "outputs": [],
   "source": [
    "ENC_EMB_DIM = 64\n",
    "DEC_EMB_DIM = 64\n",
    "ENC_HID_DIM = 256\n",
    "DEC_HID_DIM = 256\n",
    "ATTN_DIM = 64\n",
    "ENC_DROPOUT = 0.1\n",
    "DEC_DROPOUT = 0.1\n",
    "\n",
    "encoder = Encoder(input_dim=input_lang.n_words,\n",
    "                  enc_hid_dim=ENC_HID_DIM,\n",
    "                  emb_dim=ENC_EMB_DIM,\n",
    "                  dec_hid_dim=DEC_HID_DIM,\n",
    "                  dropout=ENC_DROPOUT\n",
    "                  ).to(device=device)\n",
    "decoder = Decoder(output_dim=output_lang.n_words,\n",
    "                  dec_hid_dim=DEC_HID_DIM,\n",
    "                  attention=Attention(enc_hid_dim=ENC_HID_DIM, dec_hid_dim=DEC_HID_DIM, attn_dim=ATTN_DIM),\n",
    "                  emb_dim=DEC_EMB_DIM,\n",
    "                  enc_hid_dim=ENC_HID_DIM,\n",
    "                  dropout=DEC_DROPOUT\n",
    "                  ).to(device=device)\n",
    "seq2seq = Seq2Seq(encoder, decoder).to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 11, 17859])\n"
     ]
    }
   ],
   "source": [
    "# Trying with sample batch\n",
    "outputs = seq2seq(source=sample_batch['source'], target=sample_batch['target'])\n",
    "print(outputs.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's overview our models general structure and parameter size."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "outputs": [
    {
     "data": {
      "text/plain": "==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nSeq2Seq                                  --                        --\nEncoder: 1-1                           [9, 128, 512]             --\n    Embedding: 2-1                    [128, 9, 64]              683,072\n    Dropout: 2-2                      [128, 9, 64]              --\n    GRU: 2-3                          [9, 128, 512]             494,592\n    Linear: 2-4                       [128, 256]                131,328\nDecoder: 1-2                           --                        --\n    Embedding: 2-5                    [1, 128, 64]              1,142,976\n    Dropout: 2-6                      [1, 128, 64]              --\n    Embedding: 2-7                    --                        (recursive)\n    Attention: 2-8                    [128, 9]                  --\n        Linear: 3-1                  [128, 9, 64]              49,216\n    GRU: 2-9                          [1, 128, 256]             640,512\n    Linear: 2-10                      [128, 17859]              14,876,547\n    Embedding: 2-11                   [1, 128, 64]              (recursive)\n    Dropout: 2-12                     [1, 128, 64]              --\n    Attention: 2-13                   [128, 9]                  (recursive)\n        Linear: 3-2                  [128, 9, 64]              (recursive)\n    GRU: 2-14                         [1, 128, 256]             (recursive)\n    Linear: 2-15                      [128, 17859]              (recursive)\n    Embedding: 2-16                   [1, 128, 64]              (recursive)\n    Dropout: 2-17                     [1, 128, 64]              --\n    Attention: 2-18                   [128, 9]                  (recursive)\n        Linear: 3-3                  [128, 9, 64]              (recursive)\n    GRU: 2-19                         [1, 128, 256]             (recursive)\n    Linear: 2-20                      [128, 17859]              (recursive)\n    Embedding: 2-21                   [1, 128, 64]              (recursive)\n    Dropout: 2-22                     [1, 128, 64]              --\n    Attention: 2-23                   [128, 9]                  (recursive)\n        Linear: 3-4                  [128, 9, 64]              (recursive)\n    GRU: 2-24                         [1, 128, 256]             (recursive)\n    Linear: 2-25                      [128, 17859]              (recursive)\n    Embedding: 2-26                   [1, 128, 64]              (recursive)\n    Dropout: 2-27                     [1, 128, 64]              --\n    Attention: 2-28                   [128, 9]                  (recursive)\n        Linear: 3-5                  [128, 9, 64]              (recursive)\n    GRU: 2-29                         [1, 128, 256]             (recursive)\n    Linear: 2-30                      [128, 17859]              (recursive)\n    Embedding: 2-31                   [1, 128, 64]              (recursive)\n    Dropout: 2-32                     [1, 128, 64]              --\n    Attention: 2-33                   [128, 9]                  (recursive)\n        Linear: 3-6                  [128, 9, 64]              (recursive)\n    GRU: 2-34                         [1, 128, 256]             (recursive)\n    Linear: 2-35                      [128, 17859]              (recursive)\n    Embedding: 2-36                   [1, 128, 64]              (recursive)\n    Dropout: 2-37                     [1, 128, 64]              --\n    Attention: 2-38                   [128, 9]                  (recursive)\n        Linear: 3-7                  [128, 9, 64]              (recursive)\n    GRU: 2-39                         [1, 128, 256]             (recursive)\n    Linear: 2-40                      [128, 17859]              (recursive)\n    Embedding: 2-41                   [1, 128, 64]              (recursive)\n    Dropout: 2-42                     [1, 128, 64]              --\n    Attention: 2-43                   [128, 9]                  (recursive)\n        Linear: 3-8                  [128, 9, 64]              (recursive)\n    GRU: 2-44                         [1, 128, 256]             (recursive)\n    Linear: 2-45                      [128, 17859]              (recursive)\n==========================================================================================\nTotal params: 18,018,243\nTrainable params: 18,018,243\nNon-trainable params: 0\nTotal mult-adds (G): 16.62\n==========================================================================================\nInput size (MB): 0.02\nForward/backward pass size (MB): 24.78\nParams size (MB): 72.07\nEstimated Total Size (MB): 96.87\n=========================================================================================="
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(seq2seq, input_size=[(128, 9), (128, 8)], dtypes=[torch.long, torch.long])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training Loop\n",
    "\n",
    "Since we have the Seq2Seq model, it will be easier to write training scripts because we already decoupled model logic from the training loop.\n",
    "\n",
    "train_epoch function trains the model for 1 epoch. To do so:\n",
    "- the model is switched to train mode\n",
    "- iterating dataloader for every batch in it\n",
    "    - resetting optimizer\n",
    "    - getting output from the model\n",
    "    - calculating the loss\n",
    "    - backpropagation with gradient clipping to have more stable training\n",
    "    - keeping track of loss for the epoch\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "outputs": [],
   "source": [
    "# tell them more about CrossEntropy loss why it is useful in this case\n",
    "def train_epoch(model: nn.Module,\n",
    "                iterator: DataLoader,\n",
    "                optimizer: optim.Optimizer,\n",
    "                criterion: nn.Module,\n",
    "                clip: float):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in iterator:\n",
    "        src, trg = batch['source'].to(device=device), batch['target'].to(device=device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(src, trg)\n",
    "        outputs = outputs.view(-1, outputs.shape[-1])\n",
    "        trg = trg.view(-1)\n",
    "        loss = criterion(outputs, trg)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Evaluate is very similar to training epoch but without backpropagation, since it is only used for validation and test dataloaders."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model: nn.Module,\n",
    "             iterator: DataLoader,\n",
    "             criterion: nn.Module):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    for batch in iterator:\n",
    "        src, trg = batch['source'].to(device=device), batch['target'].to(device=device)\n",
    "        outputs = model(src, trg, teacher_forcing_ratio=0)  #turn off teacher forcing\n",
    "        outputs = outputs.view(-1, outputs.shape[-1])\n",
    "        trg = trg.view(-1)\n",
    "        loss = criterion(outputs, trg)\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Helper methods"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Measures time for an epoch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "outputs": [],
   "source": [
    "def epoch_time(start_time: int,\n",
    "               end_time: int):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Used for displaying loss progress during the training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "outputs": [],
   "source": [
    "def show_plot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Setting training hyperparameters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The function we are using is Cross-Entropy loss function. It is one of the most straightforward loss functions in deep learning.\n",
    "\n",
    "> Cross-entropy loss, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label. So predicting a probability of .012 when the actual observation label is 1 would be bad and result in a high loss value. A perfect model would have a log loss of 0. [^fn4]\n",
    "\n",
    "[^fn4]: https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "outputs": [],
   "source": [
    "N_EPOCHS = 5\n",
    "CLIP = 1\n",
    "model = seq2seq\n",
    "best_valid_loss = float('inf')\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_token_index)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training will happen here. First we will train the model for an epoch then evaluate for validation dataset. In the meantime we will keep track of time, loss and perplexity.\n",
    "\n",
    "Perplexity is an indicator of how well our model performs its prediction. If you'd like to know more about it [wikipedia page](https://en.wikipedia.org/wiki/Perplexity) is pretty good source :)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 1m 25s\n",
      "\tTrain Loss: 3.557 | Train PPL:  35.068\n",
      "\t Val. Loss: 2.883 |  Val. PPL:  17.875\n",
      "Epoch: 02 | Time: 1m 26s\n",
      "\tTrain Loss: 2.137 | Train PPL:   8.478\n",
      "\t Val. Loss: 2.539 |  Val. PPL:  12.667\n",
      "Epoch: 03 | Time: 1m 28s\n",
      "\tTrain Loss: 1.683 | Train PPL:   5.381\n",
      "\t Val. Loss: 2.408 |  Val. PPL:  11.108\n",
      "Epoch: 04 | Time: 1m 28s\n",
      "\tTrain Loss: 1.430 | Train PPL:   4.179\n",
      "\t Val. Loss: 2.379 |  Val. PPL:  10.796\n",
      "Epoch: 05 | Time: 1m 28s\n",
      "\tTrain Loss: 1.270 | Train PPL:   3.561\n",
      "\t Val. Loss: 2.333 |  Val. PPL:  10.306\n",
      "| Test Loss: 2.292 | Test PPL:   9.894 |\n"
     ]
    }
   ],
   "source": [
    "epoch_losses = []\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    train_loss = train_epoch(model, train_dataloader, optimizer, criterion, CLIP)\n",
    "    epoch_losses.append(train_loss)\n",
    "    valid_loss = evaluate(model, val_dataloader, criterion)\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "\n",
    "test_loss = evaluate(model, test_dataloader, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Even 5 epochs of training results in a robust model due to our short length sentences and powerful model architecture."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAiu0lEQVR4nO3deXxU9b3/8dcnC3tCgCQQNiOQhCUgSNxlc0HcWKq9tfdqa39aH962Xq2211up1r1Vr9Zaq5ar1nq76K2KIopKlQCKoODCnrBDEEjYw5L9+/tjBkpjQibkzJyZyfv5eOTxmGS+c877cWA++ebMOd+POecQEZHYl+B3ABER8YYKuohInFBBFxGJEyroIiJxQgVdRCROJPm14/T0dJedne3X7kVEYtKSJUt2OucyGnrOt4KenZ3N4sWL/dq9iEhMMrNNjT2nUy4iInGiyYJuZu3M7BMz+9LMVpjZPY2M+xczWxkc8xfvo4qIyPGEcsqlEjjPOXfAzJKBD81slnNu4ZEBZpYD/Aw4xzm3x8wyw5RXREQa0WRBd4G1AQ4Ev00OftVfL+D7wO+cc3uCryn1MqSIiDQtpHPoZpZoZl8ApcBs59yiekNygVwz+8jMFprZBI9ziohIE0Iq6M65WufccKA3cLqZ5dcbkgTkAGOBbwP/Y2Zp9bdjZjeY2WIzW1xWVtaS3CIiUk+zrnJxzu0F5gD1Z+AlwAznXLVzbgNQTKDA13/9NOdcgXOuICOjwcsoRUTkBIVylUvGkdm2mbUHLgRW1xv2OoHZOWaWTuAUzHoPcx5VsucQ97y5guraunBsXkQkZoUyQ88C5pjZUuBTAufQZ5rZvWY2MTjmXWCXma0kMIP/qXNuVzgCr9pWzh8+2shzH24Ix+ZFRGKW+dXgoqCgwJ3onaLff3Ex89eUMfvHY+jTtYPHyUREopeZLXHOFTT0XEzeKXrPxCEkmHH3jBWo45KISEBMFvSeae259cJc3l9dyrsrtvsdR0QkKsRkQQe49uxsBmWlcveMlRyorPE7joiI72K2oCclJvDglHx2lFfw2HvFfscREfFdzBZ0gBF9u/BvZ/TlhQUbWL51n99xRER8FdMFHeCnFw2ka8e23DF9GbV1+oBURFqvmC/ondsnc+dlg1haso8/L2p03XcRkbgX8wUdYOIpPRmVk84j7xSxY3+F33FERHwRFwXdzLhvUj6VtXXcO3Ol33FERHzhWcei4NgrzMyZWYN3MYVTdnpHbho3gLeWbqOwSMuxi0jrE8oM/UjHolOA4cAEMzuz/iAzSwFuBuqvlR4xN4zpR7+Mjtz5xnIqqmv9iiEi4osmC7oLaKpjEcB9wEOAbyex2yYl8sDkoWzZfZjffrDGrxgiIr7wpGORmZ0K9HHOvdXEdsLe4OKs/t244tTeTJu3njU7ysOyDxGRaNTijkVmlgA8BtwWwnYi0uDijksG0rFtElOnL6dO16aLSCvhRceiFCAfKDSzjcCZwAw/Phg9oluntvzs4oF8snE3r3xW4lcMEZGIanHHIufcPudcunMu2zmXDSwEJjrnTmyxc498c2QfTsvuwi/fXsXug1V+RhERiQivOhZFnYQE44EpQymvqOHBt1f5HUdEJOySmhrgnFsKjGjg53c1Mn5sy2N5I7d7Ct8f3Y+nC9dx5cjenNmvm9+RRETCJi7uFD2e/zgvhz5d2zN1+jKqatRYWkTiV9wX9PZtErl3Uj7ryg4ybd46v+OIiIRN3Bd0gHF5mVw6NIvffrCWTbsO+h1HRCQsWkVBB7jr8sEkJybw89eXq7G0iMSlVlPQu6e24yfjc5m/Ziczl27zO46IiOdaTUEHuOasbIb17sy9M1ey73C133FERDzVqgp6YoLxwOSh7DpQyX+/W+R3HBERT7Wqgg4wtHdnvnNWNn9atIkvtuz1O46IiGc8aXBhZrea2UozW2pm75vZSeGJ643bxueSmdKWO15bRk2trk0XkfjgVYOLz4EC59ww4BXgYU9TeiylXTJ3Xz6Eldv288KCjX7HERHxhCcNLpxzc5xzh4LfLiSwzG5Um5Dfg3F5GTw2u5iv9h72O46ISIt50uCinuuAWY1sJ+wNLkJlZtw7KZ8657h7xgpfs4iIeKHFDS6OZWZXAwXAI41sJyINLkLVp2sHbj4/l/dW7mD2yh1+xxERaREvGlwAYGYXAFMJrIVe6Um6CLh+1Mnkdu/E3TNWcKiqxu84IiInrMUNLoI/HwH8nkAxLw1DzrBJTkzgwSlD2br3MI//XY2lRSR2edXg4hGgE/A3M/vCzGaEKW9YFGR35arT+vDchxtY+dV+v+OIiJwQ82uhqoKCArd4sa9d6v7J3kNVnP/oXPp268CrN55NQoL5HUlE5GvMbIlzrsGeza3uTtHGpHVow9RLB/H55r389dPNfscREWk2FfRjTBnRi7P6deOhWaspK4+Zz3VFRAAV9H9iZtw/JZ+K6jruf2ul33FERJpFBb2e/hmduHFsf9744is+XLPT7zgiIiFTQW/AD8b2J7tbB+58YzkV1bV+xxERCYkKegPaJSdy/+ShbNh5kKcK1VhaRGKDCnojzs1JZ9LwnjxTuI51ZQeafoGIiM9U0I/j55cOpm1yAj+frsbSIhL9vGpw0dbMXjaztWa2yMyyw5I2wjJS2nL7hIF8vH4X0z/f6nccEZHj8qrBxXXAHufcAODXwEOepvTRv57elxF903jgrVXsPVTldxwRkUZ50uACmAT8Mfj4FeB8M4uLe+cTEowHpwxl7+FqfjVrddMvEBHxiVcNLnoBWwCcczXAPqBbA9uJmgYXzTEoK5Xrzj2Zlz7dwuKNu/2OIyLSIE8bXISwnahqcNEcN5+fQ8/O7Zg6fTnVaiwtIlHIqwYXW4E+AGaWBHQGdnmQL2p0bJvEPZPyKdpRzrPzN/gdR0TkazxpcAHMAL4bfHwl8IGLw+v8LhzcnfGDu/Ob94vZsvtQ0y8QEYkgrxpcPAd0M7O1wK3Af4Unrv/unjiEBDPuekPXpotIdElqaoBzbikwooGf33XM4wrgm95Gi04909pz64W53P/WKt5Zvp2Lh2b5HUlEBNCdoifk2rOzGZyVyt1vrqC8otrvOCIigAr6CUlKTODBbwyltLySR98r9juOiAiggn7ChvdJ4+ozTuLFjzeyrGSf33FERFTQW+InF+XRrVNbpr6+jNo6fUAqIv5SQW+Bzu2TufOywSwt2cf/frzR7zgi0sqpoLfQ5cOyGJWTzn+/V8yO/RV+xxGRVkwFvYXMjPsm5VNVW8e9b6qxtIj4RwXdA9npHblp3ADeWraNOUWlfscRkVZKBd0jN4zpR/+Mjtz5+nIOV6mxtIhEXihrufQxszlmtjLYsejmBsZ0NrM3j+lq9L3wxI1ebZMSeWDKUEr2HOa3H6zxO46ItEKhzNBrgNucc4OBM4EfmtngemN+CKwMdjUaCzxqZm08TRoDzuzXjStO7c20eesp3lHudxwRaWVC6Vi0zTn3WfBxObCKQEOLfxoGpAS7FHUCdhP4RdDqTL10EJ3aJTF1+jLqdG26iERQs86hB5s/jwDqdyx6EhgEfAUsA252zn2tC0Ssdixqjq4d23DHxYP4dOMe/rZki99xRKQVCbmgm1kn4FXgFufc/npPXwR8AfQk0Ej6STNLrb+NWO5Y1BxXjuzN6dld+eWs1ew6UOl3HBFpJULtKZpMoJj/2Tn3WgNDvge8FmwovRbYAAz0LmZsSUgw7p+Sz4GKGh58W42lRSQyQrnKxQg0sFjlnHuskWGbgfOD47sDecB6r0LGotzuKdwwuh+vflbCx+viqhufiESpUGbo5wDXAOeZ2RfBr0vM7EYzuzE45j7gbDNbBrwP3O6c2xmmzDHjpvNy6NO1PVNfX0Zlja5NF5HwCqVj0YeANTHmK2C8V6HiRfs2idw3KZ9r//Ap0+au56bzc/yOJCJxTHeKhtnYvEwuHZrFb+esZePOg37HEZE4poIeAXddPpi2iQncqcbSIhJGKugR0D21HT+5KI/5a3Yy48uv/I4jInFKBT1Crj7zJIb17sx9M1ex77AaS4uI91TQIyQxwXhwylB2H6zkkXd1bbqIeE8FPYLye3Xmu2dn8+dFm/l88x6/44hInFFBj7DbxufRPaUdd0xfTk3t15a7ERE5YSroEdapbRK/uHwwq7bt54UFG/2OIyJxxJMGF8FxY4N3ka4ws7neR40fE/J7cN7ATB6bXczWvYf9jiMiccKTBhdmlgY8BUx0zg0Bvul10HhiZtwzcQh1znH3jBV+xxGROOFVg4t/JbDa4ubgOHVKbkKfrh245YJcZq/cwXsrtvsdR0TigFcNLnKBLmZWaGZLzOw7jbw+7htcNMd1555MXvcU7p6xgoOVrbLBk4h4yKsGF0nASOBSAs0u7jSz3PrbaC0NLkKVnJjAg9/I56t9FTz+92K/44hIjPOqwUUJ8K5z7mBw2dx5wCnexYxfI0/qyrdP78PzH21k5Vf1f0+KiITOqwYXbwDnmlmSmXUAziBwrl1CcPuEgaS1T+aO6cuoVWNpETlBnjS4cM6tAt4BlgKfAM8655aHLXWcSevQhqmXDuKLLXv56yeb/Y4jIjHKkwYXwXGPAI94Eao1mjKiF68sKeGhd1Yzfkh3MlPa+R1JRGKM7hSNEmbGfZPzqayu4/6ZOlslIs2ngh5F+md04sax/Znx5VfMK9ZlnSLSPCroUeYHY/tzcnpH7nxjORXVaiwtIqFTQY8y7ZIDjaU37TrEU3PW+h1HRGKICnoUOjcnncnDe/L03HWsLT3gdxwRiREq6FFq6qWDaZ+cyNTpy9RYWkRCooIepTJS2nL7xQNZtGE3r3221e84IhIDVNCj2LdP68upfdN44O1V7DlY5XccEYlynjW4CI49zcxqzOxKb2O2TgkJxgNThrLvcDW/mqXG0iJyfJ40uAAws0TgIeA9byO2boOyUrnu3JN5efEWPtmw2+84IhLFvGpwAXATgRUZ1dzCY7dckEOvtPb8/PVlVNWosbSINMyTBhdm1guYAjzdxOvV4OIEdGiTxD0Th1C84wDPfrje7zgiEqW8anDxOHC7c+6400c1uDhxFwzuzkVDuvPE+2vYsvuQ33FEJAp51eCiAHjJzDYCVwJPmdlkr0JKwC8uH0KiGXe+sVzXpovI13jS4MI5d7JzLts5lw28AvzAOfe6l0EFeqa158cX5lJYVMas5WosLSL/zJMGFxI5156dzeCsVO55cwXlFdV+xxGRKOJZg4tjxl/bkkByfEmJCTz4jaFMeeojHn2vmLsnDvE7kohECd0pGoOG90nj6jNO4o8fb2RpyV6/44hIlFBBj1E/nZBHeqe2aiwtIkepoMeo1HbJ3HXZYJZv3c+LH2/0O46IRAEV9Bh22bAsRuWk8+h7xWzfV+F3HBHxmQp6DDMz7p+cT3VtHfe8ucLvOCLiMxX0GHdSt47cdN4AZi3fzgerd/gdR0R8pIIeB24Y3Z8BmZ24640VHK5SY2mR1koFPQ60SUrg/sn5lOw5zG/eX+N3HBHxiScNLszs38xsqZktM7MFZnZKeOJKY87s140rR/bm2fnrKdpe7nccEfGBVw0uNgBjnHNDgfuAad7GlFDccckgUtolMXX6Mup0bbpIq+NJgwvn3ALn3J7gtwuB3l4HlaZ17diGn10yiMWb9vB/i7f4HUdEIsyTBhf1XAfMakEmaYFvjuzN6dld+eWs1ew8UOl3HBGJIK8aXBwZM45AQb+9kefVsSjMzIwHpuRzqKqGB99e5XccEYkgrxpcYGbDgGeBSc65XQ2NUceiyMjpnsINo/vx2mdbWbBup99xRCRCPGlwYWZ9gdeAa5xzxd5GlBPxo3E59Onanp9PX05lja5NF2kNvGpwcRfQjUDruS/MbHG4Akto2rdJ5L5J+azfeZBnCtVYWqQ18KTBhXPueuB6r0KJN8bmZXLpsCx+V7iWrM7t+MapvUhK1L1kIvFK7+4494vLBzOoRwr/+epSLvz1PN74YquuUReJUyrocS4zpR2v//Acfn/NSNokJnDzS19w8W/m887y7Tinwi4ST1TQWwEz46IhPZh18yie+PYIqmvruPFPS5j45EcUFpWqsIvECRX0ViQhwZh4Sk/e+/FoHrlyGHsOVXHtHz7lm898zMfrGrzSVERiiPk1OysoKHCLF+tiGD9V1dTx8uItPPnBGnbsr+TcAencOj6XU/t28TuaiDTCzJY45woafE4FXSqqa/nTwk08XbiOXQerOH9gJj++MJf8Xp39jiYi9aigS0gOVtbwwoKN/H7uOvZX1HDJ0B7cemEuAzJT/I4mIkEq6NIs+w5X89z89Tz34QYOV9cyeXgvbr4gh5O6dfQ7mkirp4IuJ2T3wSp+P3cdf/x4I9W1jn8p6M2PzsuhV1p7v6OJtFrHK+hedSwyM3vCzNYGOxed6kVw8deR9dXn/XQcV5/Rl1eWlDDukULunrGC0vIKv+OJSD1NztDNLAvIcs59ZmYpwBJgsnNu5TFjLgFuAi4BzgB+45w743jb1Qw99pTsOcSTH6zlb0tKSE40vnt2NjeO7k+Xjm38jibSarRohh5KxyJgEvCiC1gIpAV/EUgc6d2lA7+6Yhh/v3UME4b0YNq89Yx6eA6PzS5mf0W13/FEWj2vOhb1Ao7teVbC14u+GlzEiZPTO/L4VSN495bRjMpJ54n31zDqoTk8VbiWQ1U1fscTabU87VjUFDW4iC+53VN4+uqRzLzpXEae1IWH3yli9MNzeO7DDVRUaw12kUjzqmPRVqDPMd/3Dv5MWoH8Xp15/trTePXfzyK3ewr3zVzJ2EcK+fOiTVTV1PkdT6TV8KRjETAD+E7wapczgX3OuW0e5pQYMPKkrvzl+2fyl+vPoGdaO6ZOX875jxXyypISarVkr0jYhXKVy7nAfGAZcGS6dQfQF8A590yw6D8JTAAOAd9zzh33EhZd5RLfnHMUFpfx6HtFLN+6n/4ZHfnxhblckp9FQsJx+6WIyHHoxiLxjXOOd1ds59H3illTeoCBPVK4bXweFwzKJDAPEJHmaNFliyItYWZMyM/inVtG85urhlNRXcv3X1zM5KcWMH9NmdZiF/GQCrpERGKCMWl4L2bfOoaHrhjKzvJKrnnuE741bSGfbNjtdzyRuKBTLuKLyppaXv50C7/9YC1l5ZWMyknntvF5DO+T5nc0kaimc+gStQ5X1fK/CzfydOE69hyq5oJB3bltfC6DslL9jiYSlVTQJeodqKzhDx9uYNr89ZRX1HDZsCxuuSCXAZmd/I4mElVU0CVm7DtUzf/MX8/zHwXuNp0yoje3XJBDn64d/I4mEhVU0CXm7DpQydOF63hx4Sbq6hzfOq0PPzpvAFmdtRa7tG4q6BKztu+r4Hdz1vLSp5sxM64+4yT+fWx/MlLa+h1NxBctbXDxvJmVmtnyRp7vbGZvmtmXwQYY32tpYJEjenRux32T8/ngtrFMOqUnLyzYwOiH5/DwO6vZe6jK73giUSWUW/9HAwcIrHee38DzdwCdnXO3m1kGUAT0cM4d992mGbqciPVlB3j872t4c+lXdGqTxPWj+vH/zs0mpV2y39FEIqKlDS7mAce788MBKcH1XDoFx2pRbAmLfhmdeOLbI5h18yjO6t+NX/+9mFEPz+GZues4XKUle6V1C+kcerCxxcxGZugpBFZbHAikAN9yzr3VyHZuAG4A6Nu378hNmzadeHIRYGnJXh59r5i5xWWkd2rLj8b159tn9KVtUqLf0UTCosUfijZR0K8EzgFuBfoDs4FTmmqCoVMu4qVPN+7mv98tYtGG3fTs3I6bzs/hypG9SU7U6hYSX8K9ONf3gNeC/UTXAhsIzNZFIua07K68dMOZ/Pn6M8hMbcfPXlvGBY/NZfrnWotdWg8vCvpm4HwAM+sO5AHrPdiuSLOYGecMSGf6D87mue8W0KFNEj9++Usuenweby/bRp0Ku8S5UK5y+SswFkgHdgC/AJLhaHOLnsALQBZgwK+cc39qasc65SLhVlfneGfFdh6bXcza0gMM6ZnKbeNzGZentdgldunGImnVauscM77cyq9nr2Hz7kOM6JvGT8bncc6AdL+jiTSbCroIUF1bxytLSnji/TVs21fBWf26cdv4XAqyu/odTSRkKugix6ioruWvn2zmd3PWsfNAJWPzMrjtwjyG9u7sdzSRJqmgizTgUFUNL368iWfmrmPvoWouGtKdWy/MI69Hit/RRBqlgi5yHOUV1Tz/4Uaenb+eA1U1XD6sJ1eO7M3pJ3elXbJuUJLoooIuEoI9B6uYNn89f1ywkUNVtbRPTuTs/t0Ym5fB2LxMrckuUUEFXaQZDlfVsnD9LuYUlVJYVMbm3YcA6J/RkXF5mYzNy+S0k7toeQHxhQq6yAlyzrF+50EKi8ooLCpl0frdVNXW0aFNImf3Tw/O3jPo3UWzd4mM4xX0pEiHEYklZkb/jE70z+jEdeeezKGqGj5e94/Z+99X7QAgJ7MT4wZmMjY3g4LsrrRJ0hoyEnmh3Cn6PHAZUNrQ4lzBMWOBxwncQbrTOTemqR1rhi6xzjnHurIDwdl7GYs27KK61tGxTSLnDEgPFPi8DLXNE0+16JRLCA0u0oAFwATn3GYzy3TOlTYVSgVd4s3ByhoWBGfvc4vK2Lr3MAB53VMYOzCDsbmZFGR30QqQ0iLhXj73B0BP59zPmxNKBV3imXOONaUHKCwqZc7qMj7duJuaOkdK26Tg7D2DMbmZ9Ojczu+oEmPCfQ49F0g2s0ICDS5+45x7sZEgxza48GDXItHJzMjtnkJu9xRuGN2f8opqPlq7i7nFgQL/zortAAzKSmVsXgbj8jI5tW8aSZq9Swt4MUN/EiggsIRue+Bj4FLnXPHxtqkZurRWzjmKdpRTWFTGnNWlLNm0JzB7b5fEqJx0xuYFPlzNTNXsXb4u3DP0EmCXc+4gcNDM5gGnAMct6CKtlZkxsEcqA3ukcuOY/uyvqOajNTsDBb6olLeXBWbvQ3r+Y/Y+vI9m79I0Lwr6G8CTZpYEtAHOAH7twXZFWoXUdslcPDSLi4dm4Zxj1bbyox+sPjN3Pb+bs47UdkmMzg3csTomN4OMlLZ+x5Yo1GRBP7bBhZmVUK/BhXNulZm9AywF6oBnnXPLwxdZJH6ZGYN7pjK4Zyo/HDeAfYer+XDNTgqLSiksLmPm0m0ADO3VmXF5GYwJzt4TE9SwQ3SnqEjMqKtzrNy2P1Dci8r4bPMe6hykdUhmdE7gjtXRuRmkd9LsPZ7p1n+ROLT3UBXzg+fe5xaXsvNAFWYwrFfnwAereRkM663Ze7xRQReJc3V1jhVf7Q8uSVDK51v24hx06ZDMmOC599G5GXTt2MbvqNJCKugircyeg1XMW1MWnL2XsftgYPZ+Su+04IqRGQzt1ZkEzd5jjgq6SCtWV+dYunVf4K7VojKWlgRm7906tmFMbgZj8jIYnZNBF83eY4IKuogctetA5dHZ+7ziMvYcqibBYHiftKPrvQ/pmarZe5RSQReRBtXWOb4s2Xt0vfelJfsASO/UljG5GYwbmMGoARl07pDsc1I5QgVdREJSVl7JvOIyCosDs/d9h6tJTDBO7Zt29KamIT1TMdPs3S8q6CLSbDW1dUdn73OKSlm+dT8AmSlHZu+ZnJuTTmo7zd4jSQVdRFqstLyCuUX/mL2XV9SQmGCM7NuFESelMahHKoOyUumX0VFrvodRSxtcNNmxKDjuNAIrLV7lnHulqVAq6CKxq6a2js+37KWwqJR5xTsp2l5OVW0dAG0SExiQ2YlBWakMykphUFYqA3uk0E13sHoirB2LgmMSgdlABfC8CrpI61JdW8f6soOs2rafVdv3s2pbOau37ae0vPLomMyUtgw8UuQ1mz9hLVo+1zk3L7ge+vHcBLwKnNb8eCIS65ITE8jrkUJejxQm0+voz3cdqGT19nJWbdvPym37Wb2tnD+s26XZfJi0ePlcM+sFTAHG0URBV8cikdalW6e2nDOgLecMSD/6s4Zm8/PXlPHqZyVHx2g2f2K8WA/9ceB251xdU5cyOeemAdMgcMrFg32LSIzRbD58vCjoBcBLwWKeDlxiZjXOudc92LaItBKazbdciwu6c+7kI4/N7AUCvUdfb+l2RUQ0m2+eFncsCms6EZEGaDbfMN1YJCJxraHZ/NrSAzE7m2/RZYsiIrGsNc3mVdBFpNWJ13PzKugiIkGxPptXQRcROY5Yms2roIuInICWzOZvGN2P60f18zyTCrqIiEdCnc1npIRnlq6CLiISZg3N5sMheq+/ERGRZmmyoJvZ82ZWambLG3n+38xsqZktM7MFZnaK9zFFRKQpoczQXwAmHOf5DcAY59xQ4D6CqymKiEhktbjBhXNuwTHfLgR6e5BLRESayetz6NcBsxp70sxuMLPFZra4rKzM412LiLRunhV0MxtHoKDf3tgY59w051yBc64gIyPDq12LiAgeXbZoZsOAZ4GLnXO7vNimiIg0T4tn6GbWF3gNuMY5V9zySCIiciKaXA/92AYXwA7qNbgws2eBK4BNwZfUNLZWb73tlh3zmuZKB3ae4GvDKVpzQfRmU67mUa7micdcJznnGjxn7VuDi5Yws8Wh/NKItGjNBdGbTbmaR7map7Xl0p2iIiJxQgVdRCROxGpBj9a7UaM1F0RvNuVqHuVqnlaVKybPoYuIyNfF6gxdRETqUUEXEYkTUV3QzWyCmRWZ2Voz+68Gnm9rZi8Hn190vEXEIpzrWjMrM7Mvgl/XRyhXU0sdm5k9Ecy91MxOjZJcY81s3zHH664IZOpjZnPMbKWZrTCzmxsYE/HjFWKuiB+v4H7bmdknZvZlMNs9DYyJ+HsyxFx+vScTzexzM5vZwHPeHyvnXFR+AYnAOqAf0Ab4Ehhcb8wPgGeCj68CXo6SXNcCT/pwzEYDpwLLG3n+EgKLpxlwJrAoSnKNBWZG+FhlAacGH6cAxQ38O0b8eIWYK+LHK7hfAzoFHycDi4Az643x4z0ZSi6/3pO3An9p6N8rHMcqmmfopwNrnXPrnXNVwEvApHpjJgF/DD5+BTjfzCwKcvnCOTcP2H2cIZOAF13AQiDNzLKiIFfEOee2Oec+Cz4uB1bBMU0gAyJ+vELM5YvgcTgQ/DY5+FX/qoqIvydDzBVxZtYbuJTAOlcN8fxYRXNB7wVsOeb7Er7+H/voGOdcDbAP6BYFuQCuCP6Z/oqZ9QlzplCFmt0PZwX/ZJ5lZkMiuePgn7ojCMzsjuXr8TpOLvDpeAVPIXwBlAKznXONHrMIvidDyQWRf08+DvwnUNfI854fq2gu6LHsTSDbOTcMmM0/fgtLwz4jsD7FKcBvgdcjtWMz6wS8CtzinNsfqf02pYlcvh0v51ytc244gUY2p5tZfqT2fTwh5Iroe9LMLgNKnXNLwrmf+qK5oG8Fjv0t2jv4swbHmFkS0BkI9/K9TeZyzu1yzlUGv30WGBnmTKEK5ZhGnHNu/5E/mZ1zbwPJZhbe9uiAmSUTKJp/ds691sAQX45XU7n8Ol71MuwF5vD19pR+vCebzOXDe/IcYKKZbSRwWvY8M/tTvTGeH6toLuifAjlmdrKZtSHwocGMemNmAN8NPr4S+MAFP2HwM1e986wTCZwHjQYzgO8Er944E9jnnNvmdygz63Hk3KGZnU7g/2VYi0Bwf88Bq5xzjzUyLOLHK5Rcfhyv4L4yzCwt+Lg9cCGwut6wiL8nQ8kV6fekc+5nzrnezrlsAjXiA+fc1fWGeX6sPGlwEQ7OuRoz+xHwLoErS553zq0ws3uBxc65GQT+4/+vma0l8KHbVVGS6z/MbCJQE8x1bbhzwT8vdWxmJdRb6hh4m8CVG2uBQ8D3oiTXlcC/m1kNcBi4KgK/mM8BrgGWBc+9AtwB9D0mlx/HK5RcfhwvCFyB80czSyTwS+T/nHMz/X5PhpjLl/dkfeE+Vrr1X0QkTkTzKRcREWkGFXQRkTihgi4iEidU0EVE4oQKuohInFBBFxGJEyroIiJx4v8DdwOh1Qyl9ccAAAAASUVORK5CYII=\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_plot(epoch_losses)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inference\n",
    "\n",
    "Inference is very similar to Seq2Seq models forward pass. However, here we are only interested in getting the output of one \"sentence\" or \"phrase\". We need to be consistent with our training when inputting. Hence, we are going to add SOS & EOS tokens to the input.\n",
    "\n",
    "Unlike the training, we are keeping the attention values so that later we can create the attention map visualizations."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def infer(model: Seq2Seq, sentence: str, max_len: int):\n",
    "    sentence = ' '.join([sos_token, normalize_string(sentence), eos_token])\n",
    "    input_tensor = tensor_from_sentence(input_lang, sentence)\n",
    "    padded_input_tensor = torch.full([longest_input_length], pad_token_index, dtype=torch.long, device=device)\n",
    "    padded_input_tensor[:len(input_tensor)] = input_tensor\n",
    "    encoder_output, hidden = model.encoder(padded_input_tensor.view(1, -1))\n",
    "    decoder_attentions = torch.zeros(max_len, longest_input_length, device=device)\n",
    "    decoded_words = []\n",
    "    output = torch.tensor([sos_token_index], dtype=torch.long, device=device)\n",
    "    for t in range(max_len):\n",
    "        output, hidden, decoder_attention = model.decoder.forward(output, hidden, encoder_output)\n",
    "        decoder_attentions[t] = decoder_attention[0, :]\n",
    "        top_v, top_i = output.topk(1)\n",
    "        top_i = top_i.squeeze(1)\n",
    "        if top_i == eos_token_index:\n",
    "            decoded_words.append(eos_token)\n",
    "            break\n",
    "        else:\n",
    "            decoded_words.append(output_lang.index2word[top_i.item()])\n",
    "        output = top_i.detach()\n",
    "\n",
    "    return decoded_words, decoder_attentions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Function for random inference. We can cross check from google translate :) to test our model by our eyes."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "outputs": [],
   "source": [
    "def infer_randomly(model, n=5):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = infer(model, pair[0], longest_output_length)\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> he saw the girl .\n",
      "= il a vu la fille .\n",
      "< il a vu la fille . <end>\n",
      "\n",
      "> i just wanted to check my email .\n",
      "= je voulais juste verifier mes courriels .\n",
      "< je voulais juste verifier mon emails . <end>\n",
      "\n",
      "> who owns this house ?\n",
      "= qui est proprietaire de cette maison ?\n",
      "< qui a proprietaire cette maison ? <end>\n",
      "\n",
      "> she is aggressive .\n",
      "= elle est agressive .\n",
      "< elle est agressive . <end>\n",
      "\n",
      "> it was an awful week .\n",
      "= ca a ete une semaine atroce .\n",
      "< c etait une une semaine atroce . <end>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "infer_randomly(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Visualizing raw attention map for a random pair."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tom hates being told to hurry .', 'tom deteste qu on lui dise de se depecher .']\n",
      "['tom', 'deteste', 'etre', 'il', 'a', 'a', 'depecher', '.', '<end>']\n"
     ]
    },
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x7f88607aa370>"
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 314.182x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_pair = random.choice(pairs)\n",
    "print(test_pair)\n",
    "decoded_words, decoder_attentions = infer(model, test_pair[0], longest_output_length)\n",
    "print(decoded_words)\n",
    "plt.matshow(decoder_attentions.cpu().numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Prettifying our attention map to label indices and labels."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "outputs": [],
   "source": [
    "def show_attention(input_sentence, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.cpu().numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' '), rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "outputs": [],
   "source": [
    "def infer_and_show_attention(input_sentence):\n",
    "    output_words, attentions = infer(model, input_sentence, longest_output_length)\n",
    "    output = ' '.join(output_words)\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', output)\n",
    "    attentions = attentions[:len(output.split(' ')), 1:len(input_sentence.split(' ')) + 1]\n",
    "    show_attention(' '.join([ normalize_string(input_sentence), eos_token]), output_words, attentions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = she s five years younger than me .\n",
      "output = elle a cinq ans de jeune que moi . <end>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11060/3299921217.py:9: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_xticklabels([''] + input_sentence.split(' '), rotation=90)\n",
      "/tmp/ipykernel_11060/3299921217.py:10: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_yticklabels([''] + output_words)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "infer_and_show_attention('she s five years younger than me .')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Optional Homeworks\n",
    "- Using BLEU score to evaluate our models translation abilities.\n",
    "- Train the model with different hyperparameters\n",
    "- Try to use the same architecture with different dataset, Q/A, Chat/Response, Human/Machine(e.g. IOT commands)\n",
    "- Try to use different models for encoder - decoder\n",
    "    - try it with different attention mechanisms\n",
    "    - try it without attention and compare results\n",
    "- Training as an autoencoder\n",
    "- After training try to use encoder as a backbone for other downstreams tasks, such emotion recognition.\n",
    "- Instead of training embedding layer from scratch use pretrained embeddings such as word2vec - GloVe"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "sources:\n",
    "- https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "- https://pytorch.org/tutorials/beginner/torchtext_translation_tutorial.html\n",
    "- https://sh-tsang.medium.com/review-neural-machine-translation-by-jointly-learning-to-align-and-translate-3b381fc032e3\n",
    "- https://arxiv.org/pdf/1409.0473.pdf\n",
    "- https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html\n",
    "- https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/\n",
    "- https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "- https://github.com/SethHWeidman/pytorch-seq2seq/"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-lit-template",
   "language": "python",
   "display_name": "lit_template"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}